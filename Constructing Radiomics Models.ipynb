{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35d2e8f",
   "metadata": {},
   "source": [
    "# Constructing Radiomics Model\n",
    "\n",
    "Data storage form:\n",
    "1. `images`form，Store all the CT, MRI and other data of the study subjects.The image ends with image1.nii.gz.\n",
    "2. `masks`form, Store all ROIs, which were named same as data in images folder.\n",
    "3. `label.txt`form，Labels for each patient, such as benign and malignant tumors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7bdf17",
   "metadata": {},
   "source": [
    "## 1.Data check\n",
    "\n",
    "1. `mydir`: Path for storing data.\n",
    "2. `labelf`: Annotated information file for each sample.\n",
    "3. `labels`: To let the AI system learn the target, such as the benign and malignant tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "from onekey_algo import OnekeyDS as okds\n",
    "from onekey_algo import get_param_in_cwd\n",
    "\n",
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# mydir = r'Path for storing data'\n",
    "mydir = get_param_in_cwd('radio_dir') or okds.ct\n",
    "if mydir == okds.ct:\n",
    "    print(f'Please check path for storing data')\n",
    "group_info = get_param_in_cwd('dataset_column') or 'group'\n",
    "# labelf = r'Path for Annotated information file for each sample'\n",
    "labelf = get_param_in_cwd('label_file') or os.path.join(mydir, 'label.csv')\n",
    "# Read the label data column name\n",
    "labels = [get_param_in_cwd('task_column') or 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from onekey_algo.custom.components.Radiology import diagnose_3d_image_mask_settings, get_image_mask_from_dir\n",
    "\n",
    "images, masks = get_image_mask_from_dir(mydir, images='images', masks='masks')\n",
    "\n",
    "iagnose_3d_image_mask_settings(images, masks)\n",
    "print(f'Get{len(images)}samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d5875",
   "metadata": {},
   "source": [
    "# Extracting radiomics features\n",
    "\n",
    "The encapsulated interface.\n",
    "\n",
    "```python\n",
    "def extract(self, images: Union[str, List[str]], \n",
    "            masks: Union[str, List[str]], labels: Union[int, List[int]] = 1, settings=None)\n",
    "\"\"\"\n",
    "    * images: List structure, a list of images to be extracted.\n",
    "    * masks: List structure, mask corresponding to the image to be extracted, and Images must correspond one to one.\n",
    "    * labels: Extracts features that label what. The default is to extract label=1.\n",
    "    * settings: Other parameters for extracting features. The default is None.\n",
    "\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "def get_label_data_frame(self, label: int = 1, column_names=None, images='images', masks='labels')\n",
    "\"\"\"\n",
    "    * label: Obtains the features of the corresponding label.\n",
    "    * columns_names: The default value is None. Use the column name specified by the program.\n",
    "\"\"\"\n",
    "```\n",
    "    \n",
    "```python\n",
    "def get_image_mask_from_dir(root, images='images', masks='labels')\n",
    "\"\"\"\n",
    "    * root: indicates the directory from which features are to be extracted.\n",
    "    * images: The folder name of the raw data in the root directory.\n",
    "    * masks: Name of the folder in the root directory where the data is labeled.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b88fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    " \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from onekey_algo.custom.components.Radiology import ConventionalRadiomics\n",
    "\n",
    "if os.path.exists('results/rad_features.csv'):\n",
    "    rad_data = pd.read_csv('results/rad_features.csv', header=0)\n",
    "else:\n",
    "    param_file = r'./custom_settings/exampleCT.yaml'\n",
    "    radiomics = ConventionalRadiomics(param_file, correctMask=True)\n",
    "    radiomics.extract(images, masks)\n",
    "    rad_data = radiomics.get_label_data_frame(label=1)\n",
    "    rad_data.columns = [c.replace('-', '_') for c in rad_data.columns]\n",
    "    rad_data.to_csv('results/rad_features.csv', header=True, index=False)\n",
    "rad_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2b71c",
   "metadata": {},
   "source": [
    "## Merge label data and radiomics features data\n",
    "\n",
    "Data is stored in csv format.\n",
    "\n",
    "label_data is required to be in a DataFrame format, including ID columns and subsequent labels columns, which can be multiple columns and support Multi-Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.read_csv(labelf)\n",
    "label_data['ID'] = label_data['ID'].map(lambda x: f\"{x}.nii.gz\" if not (f\"{x}\".endswith('.nii.gz') or  f\"{x}\".endswith('.nii')) else x)\n",
    "label_data = label_data[['ID', 'group'] + labels]\n",
    "combined_data = pd.merge(rad_data, label_data, on=['ID'], how='inner')\n",
    "ids = combined_data['ID']\n",
    "combined_data = combined_data.drop(['ID'], axis=1)\n",
    "print(combined_data[labels].value_counts())\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a13a21",
   "metadata": {},
   "source": [
    "## Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import normalize_df\n",
    "data = normalize_df(combined_data, not_norm=labels, group=group_info)\n",
    "data = data.dropna(axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad794054",
   "metadata": {},
   "source": [
    "### Selecting radiomics features by t test or u test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b04bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from onekey_algo.custom.components.stats import clinic_stats\n",
    "\n",
    "stats = clinic_stats(data[data['group'] == 'train'], stats_columns=list(data.columns[0:-2]), label_column=labels[0], \n",
    "                     continuous_columns=list(data.columns[0:-2]))\n",
    "\n",
    "pvalue = 0.05\n",
    "sel_feature = list(stats[stats['pvalue'] < pvalue]['feature_name']) + labels + [group_info]\n",
    "data = data[sel_feature]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd361f98",
   "metadata": {},
   "source": [
    "### Selecting radiomics features by calculating correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e09b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = data[data['group'] == 'train'][[c for c in data.columns if c not in labels]].corr('pearson')\n",
    "\n",
    "from onekey_algo.custom.components.comp1 import select_feature\n",
    "sel_feature = select_feature(pearson_corr, threshold=0.9, topn=10, verbose=False)\n",
    "sel_feature = sel_feature + labels + [group_info]\n",
    "sel_feature\n",
    "\n",
    "sel_data = data[sel_feature]\n",
    "sel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e31ba9",
   "metadata": {},
   "source": [
    "## Partition data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a923e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onekey_algo.custom.components as okcomp\n",
    "\n",
    "n_classes = 2\n",
    "train_data = sel_data[(sel_data[group_info] == 'train')]\n",
    "train_ids = ids[train_data.index]\n",
    "train_data = train_data.reset_index()\n",
    "train_data = train_data.drop('index', axis=1)\n",
    "y_data = train_data[labels]\n",
    "X_data = train_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "test_data = sel_data[sel_data[group_info] != 'train']\n",
    "test_ids = ids[test_data.index]\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.drop('index', axis=1)\n",
    "y_test_data = test_data[labels]\n",
    "X_test_data = test_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "y_all_data = sel_data[labels]\n",
    "X_all_data = sel_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "column_names = X_data.columns\n",
    "print(f\"training set samples：{X_data.shape}, test set samples：{X_test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548eaee",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = okcomp.comp1.lasso_cv_coefs(X_data, y_data, column_names=None)\n",
    "plt.savefig(f'img/Rad_feature_lasso.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "okcomp.comp1.lasso_cv_efficiency(X_data, y_data, points=50)\n",
    "plt.savefig(f'img/Rad_feature_mse.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02f30b",
   "metadata": {},
   "source": [
    "### Penalty coefficient\n",
    "\n",
    "The cross-validated penalty coefficient is used as the basis for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "models = []\n",
    "for label in labels:\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf.fit(X_data, y_data[label])\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bda574",
   "metadata": {},
   "source": [
    "### Radiomics features after selection\n",
    "\n",
    "Coefficients that are relatively high when screened by Lasso are used as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "COEF_THRESHOLD = 1e-6\n",
    "scores = []\n",
    "selected_features = []\n",
    "for label, model in zip(labels, models):\n",
    "    feat_coef = [(feat_name, coef) for feat_name, coef in zip(column_names, model.coef_) \n",
    "                 if COEF_THRESHOLD is None or abs(coef) > COEF_THRESHOLD]\n",
    "    selected_features.append([feat for feat, _ in feat_coef])\n",
    "    formula = ' '.join([f\"{coef:+.6f} * {feat_name}\" for feat_name, coef in feat_coef])\n",
    "    score = f\"{label} = {model.intercept_} {'+' if formula[0] != '-' else ''} {formula}\"\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb884b8e",
   "metadata": {},
   "source": [
    "### Radiomics feature weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a22e86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_coef = sorted(feat_coef, key=lambda x: x[1])\n",
    "feat_coef_df = pd.DataFrame(feat_coef, columns=['feature_name', 'Coefficients'])\n",
    "feat_coef_df.plot(x='feature_name', y='Coefficients', kind='barh')\n",
    "\n",
    "plt.savefig(f'img/Rad_feature_weights.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10064550",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data[selected_features[0]]\n",
    "X_test_data = X_test_data[selected_features[0]]\n",
    "X_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7e7da",
   "metadata": {},
   "source": [
    "## Construsting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = 'SVM', 'KNN', 'RandomForest','DecisionTree', 'XGBoost', 'LightGBM', 'NaiveBayes', 'GradientBoosting', 'LR', 'MLP']\n",
    "models = okcomp.comp1.create_clf_model(model_names)\n",
    "model_names = list(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369ea2d",
   "metadata": {},
   "source": [
    "### Cross verification\n",
    "\n",
    "Args:\n",
    "    test_size: test set ratio, effective only when cv=False\n",
    "    cv: Whether to cross validate.\n",
    "    n_trails: When cv=True, it is the n_fold for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "results = okcomp.comp1.get_bst_split(X_data, y_data, models, test_size=0.2, metric_fn=roc_auc_score, n_trails=10, cv=True, random_state=0)\n",
    "_, (X_train_sel, X_test_sel, y_train_sel, y_test_sel) = results['results'][results['max_idx']]\n",
    "X_train_sel, X_test_sel, y_train_sel, y_test_sel = X_data, X_test_data, y_data, y_test_data\n",
    "trails, _ = zip(*results['results'])\n",
    "cv_results = pd.DataFrame(trails, columns=model_names)\n",
    "# Visualizing the effects of the model on data partitioning.\n",
    "sns.boxplot(data=cv_results)\n",
    "plt.ylabel('AUC %')\n",
    "plt.xlabel('Model Name')\n",
    "plt.savefig(f'img/Rad_model_cv.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158f70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from onekey_algo.custom.components.comp1 import plot_feature_importance, plot_learning_curve, smote_resample\n",
    "targets = []\n",
    "os.makedirs('models', exist_ok=True)\n",
    "for l in labels:\n",
    "    new_models = list(okcomp.comp1.create_clf_model_none_overfit(model_names).values())\n",
    "    for mn, m in zip(model_names, new_models):\n",
    "        X_train_smote, y_train_smote = X_train_sel, y_train_sel\n",
    "        m.fit(X_train_smote, y_train_smote[l])\n",
    "        joblib.dump(m, f'models/{mn}_{l}.pkl') \n",
    "        plot_feature_importance(m, selected_features[0], save_dir='img')\n",
    "        \n",
    "        plot_learning_curve(m, X_train_sel, y_train_sel, title=f'Learning Curve {mn}')\n",
    "        plt.savefig(f\"img/Rad_{mn}_learning_curve.svg\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "    targets.append(new_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88194089",
   "metadata": {},
   "source": [
    "## Forecast result\n",
    "\n",
    "* predictions，two-dimensional data, predictions for each model corresponding to each label.\n",
    "* pred_scores，two-dimensional data, the predicted probability value of each model corresponding to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from onekey_algo.custom.components.delong import calc_95_CI\n",
    "from onekey_algo.custom.components.metrics import analysis_pred_binary\n",
    "\n",
    "predictions = [[(model.predict(X_train_sel), model.predict(X_test_sel)) \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "pred_scores = [[(model.predict_proba(X_train_sel), model.predict_proba(X_test_sel)) \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "\n",
    "metric = []\n",
    "pred_sel_idx = []\n",
    "for label, prediction, scores in zip(labels, predictions, pred_scores):\n",
    "    pred_sel_idx_label = []\n",
    "    for mname, (train_pred, test_pred), (train_score, test_score) in zip(model_names, prediction, scores):\n",
    "         acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], \n",
    "                                                                                              train_score[:, 1])\n",
    "        ci = f\"{ci[0]:.4f} - {ci[1]:.4f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f\"{label}-train\"))\n",
    "                 \n",
    "        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test_sel[label], \n",
    "                                                                                              test_score[:, 1])\n",
    "        ci = f\"{ci[0]:.4f} - {ci[1]:.4f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f\"{label}-test\"))\n",
    "       \n",
    "    pred_sel_idx_label.append(np.logical_or(test_score[:, 0] >= thres, test_score[:, 1] >= thres))\n",
    "    \n",
    "    pred_sel_idx.append(pred_sel_idx_label)\n",
    "metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',\n",
    "                                                   'Sensitivity', 'Specificity', \n",
    "                                                   'PPV', 'NPV', 'Precision', 'Recall', 'F1',\n",
    "                                                   'Threshold', 'Task'])\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50bc4d4",
   "metadata": {},
   "source": [
    "### Draw the accuracy of the model bar chart and line chart curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f86dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "sns.barplot(x='model_name', y='Accuracy', data=metric, hue='Task')\n",
    "plt.subplot(212)\n",
    "sns.lineplot(x='model_name', y='Accuracy', data=metric, hue='Task')\n",
    "plt.savefig(f'img/Rad_model_acc.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5585c2",
   "metadata": {},
   "source": [
    "### Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "    \n",
    "         plt.figure(figsize=(8, 8))\n",
    "        for pred_score, label in zip(pred_scores, labels):\n",
    "            okcomp.comp1.draw_roc([np.array(y_train_sel[label]), np.array(y_test_sel[label])], \n",
    "                                  pred_score[sel_model_idx], \n",
    "                                  labels=['Train', 'Test'], title=f\"Model: {sm}\")\n",
    "            plt.savefig(f'img/Rad_model_{sm}_roc.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            pred_test_scores.append(pred_score[sel_model_idx][0])\n",
    "    okcomp.comp1.draw_roc([np.array(y_train_sel[label])] * len(pred_test_scores), \n",
    "                          pred_test_scores, \n",
    "                          labels=sel_model, title=f\"Model AUC\")\n",
    "    plt.savefig(f'img/model_roc_train.svg', bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            pred_test_scores.append(pred_score[sel_model_idx][1])\n",
    "    okcomp.comp1.draw_roc([np.array(y_test_sel[label])] * len(pred_test_scores), \n",
    "                          pred_test_scores, \n",
    "                          labels=sel_model, title=f\"Model AUC\")\n",
    "    plt.savefig(f'img/model_roc_test.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e955e3",
   "metadata": {},
   "source": [
    "### DCA curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab01b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import plot_DCA\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            okcomp.comp1.plot_DCA(pred_score[sel_model_idx][0][:,1], np.array(y_train_sel[label]),\n",
    "                                  title=f'Model {sm} DCA')\n",
    "            plt.savefig(f'img/model_{sm}_dca_train.svg', bbox_inches = 'tight')\n",
    "            plt.show()\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            okcomp.comp1.plot_DCA(pred_score[sel_model_idx][1][:,1], np.array(y_test_sel[label]),\n",
    "                                  title=f'Model {sm} DCA')\n",
    "            plt.savefig(f'img/model_{sm}_dca_test.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da92679",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "sel_model = sel_model\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            target = targets[idx][sel_model_idx]\n",
    "            train_indexes = np.reshape(np.array(train_ids), (-1, 1)).astype(str)\n",
    "            test_indexes = np.reshape(np.array(test_ids), (-1, 1)).astype(str)\n",
    "            y_train_pred_scores = target.predict_proba(X_train_sel)\n",
    "            y_test_pred_scores = target.predict_proba(X_test_sel)\n",
    "            columns = ['ID'] + [f\"{label}-{i}\"for i in range(y_test_pred_scores.shape[1])]\n",
    "            result_train = pd.DataFrame(np.concatenate([train_indexes, y_train_pred_scores], axis=1), columns=columns)\n",
    "            result_train.to_csv(f'results/Rad_{sm}_train.csv', index=False)\n",
    "            result_test = pd.DataFrame(np.concatenate([test_indexes, y_test_pred_scores], axis=1), columns=columns)\n",
    "            result_test.to_csv(f'results/Rad_{sm}_test.csv', index=False)\n",
    "            \n",
    "label_data.to_csv('group.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb28b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
